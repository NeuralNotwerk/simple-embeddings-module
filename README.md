# Simple Embeddings Module (SEM)

ğŸš€ **A modular, cross-platform semantic search engine with intelligent chunking and GPU acceleration.**

## âœ¨ Features

- ğŸ§  **Semantic Search**: Find documents by meaning, not just keywords
- âš¡ **GPU Acceleration**: Apple Silicon MPS, NVIDIA CUDA, AMD ROCm support via PyTorch
- ğŸ“ **Intelligent Chunking**: Auto-configured based on embedding model constraints
- ğŸ”’ **Secure**: No pickle files - pure JSON serialization with orjson
- ğŸ”§ **Modular**: "Bring your own" embedding models, storage backends, and chunking strategies
- ğŸ¯ **Production Ready**: Atomic writes, backups, compression, validation
- ğŸ“Š **Scalable**: Designed for 100K+ documents by default

## ğŸš€ Quick Start

### Installation

```bash
# Option 1: Install from source
git clone https://github.com/NeuralNotwerk/simple-embeddings-module.git
cd simple-embeddings-module
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -e .

# Option 2: Install from PyPI (when available)
# Not available yet
```

### Super Simple Usage

```python
# The simplest possible semantic search - just import and go!
from simple_embeddings_module import SEMSimple

sem = SEMSimple()
sem.add_text("Machine learning is transforming software development.")
results = sem.search("AI technology")
print(results[0]['text'])  # Found it!
```

### Basic Usage

```bash
# Initialize a new database
sem-cli init --name my_docs --path ./my_indexes

# Add documents
echo "Machine learning transforms software development." > doc1.txt
echo "PyTorch provides excellent GPU acceleration." > doc2.txt
sem-cli add --files doc1.txt doc2.txt --path ./my_indexes

# Search semantically
sem-cli search "artificial intelligence" --path ./my_indexes

# Show database info
sem-cli info --path ./my_indexes
```

## ğŸ—ï¸ Architecture

### Modular Design

SEM uses a plugin-based architecture with four main component types:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embedding     â”‚    â”‚    Chunking     â”‚    â”‚    Storage      â”‚    â”‚ Serialization   â”‚
â”‚   Providers     â”‚    â”‚   Strategies    â”‚    â”‚   Backends      â”‚    â”‚   Providers     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ sentence-     â”‚    â”‚ â€¢ text          â”‚    â”‚ â€¢ local_disk    â”‚    â”‚ â€¢ orjson        â”‚
â”‚   transformers  â”‚    â”‚ â€¢ code (TODO)   â”‚    â”‚ â€¢ s3 (TODO)     â”‚    â”‚ â€¢ json (TODO)   â”‚
â”‚ â€¢ openai        â”‚    â”‚ â€¢ csv (TODO)    â”‚    â”‚ â€¢ gcs (TODO)    â”‚    â”‚                 â”‚
â”‚ â€¢ bedrock       â”‚    â”‚ â€¢ chunk_mux     â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ ollama        â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dependency Chain

The system enforces a strict dependency chain to ensure compatibility:

```
Embedding Provider â†’ Chunking Strategy â†’ Index Structure â†’ Storage Backend
```

- **Embedding Provider** determines dimensions, max sequence length, and optimal chunk sizes
- **Chunking Strategy** adapts to embedding constraints and document types
- **Index Structure** optimized for the chosen embedding and chunking combination
- **Storage Backend** handles persistence with appropriate serialization

## ğŸ¯ Core Components

### CLI Interface

```bash
# Available commands
sem-cli init     # Initialize new database
sem-cli add      # Add documents from files or text
sem-cli search   # Semantic search with configurable parameters
sem-cli info     # Show database information and statistics
sem-cli config   # Generate configuration templates
```

### Configuration System

SEM uses JSON-only configuration (no YAML, no pickles):

```json
{
  "embedding": {
    "provider": "sentence_transformers",
    "model": "all-MiniLM-L6-v2",
    "batch_size": 32
  },
  "chunking": {
    "strategy": "text",
    "boundary_type": "sentence"
  },
  "storage": {
    "backend": "local_disk",
    "path": "./indexes"
  },
  "serialization": {
    "provider": "orjson"
  }
}
```

### Intelligent Chunking

Chunking strategies automatically configure based on embedding provider capabilities:

- **Chunk Size**: 80% of embedding model's max sequence length
- **Overlap**: 10% of chunk size for context preservation
- **Boundaries**: Sentence, paragraph, or word boundaries
- **Validation**: Ensures chunks fit within embedding constraints

## ğŸ”§ Advanced Usage

### Embedding Providers

SEM supports multiple embedding providers with automatic capability detection:

#### Sentence Transformers (Local)
```python
builder.set_embedding_provider("sentence_transformers", model="all-MiniLM-L6-v2")
```
- **Models**: Any HuggingFace sentence-transformers model
- **GPU Support**: Apple Silicon MPS, NVIDIA CUDA, AMD ROCm
- **Offline**: Works without internet after model download
- **Cost**: Free

#### OpenAI Embeddings (API)
```python
builder.set_embedding_provider("openai", 
    model="text-embedding-3-small",
    api_key="your-api-key"  # or set OPENAI_API_KEY env var
)
```
- **Models**: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
- **Features**: Custom dimensions (3-small/large), high quality
- **Rate Limits**: Automatic retry with exponential backoff
- **Cost**: ~$0.00002-0.00013 per 1K tokens

#### AWS Bedrock (API)
```python
builder.set_embedding_provider("bedrock",
    model_id="amazon.titan-embed-text-v1",
    region="us-east-1"
)
```
- **Models**: Amazon Titan, Cohere Embed (English/Multilingual)
- **Authentication**: IAM roles, profiles, or explicit credentials
- **Enterprise**: AWS security, compliance, and billing
- **Cost**: Pay-per-use through AWS billing

#### Ollama Local Models (Local)
```python
builder.set_embedding_provider("ollama", 
    model="snowflake-arctic-embed2",
    auto_start_server=True
)
```
- **Models**: Any Ollama-compatible embedding model
- **Features**: Automatic server management, model downloading
- **Privacy**: Fully local processing, no data leaves your machine
- **Cost**: Free (after initial model download)

### Custom Configuration

```python
from simple_embeddings_module import SEMConfigBuilder

# Build custom configuration
builder = SEMConfigBuilder()
builder.set_embedding_provider("sentence_transformers", model="all-mpnet-base-v2")
builder.auto_configure_chunking()
builder.set_storage_backend("local_disk", path="./custom_indexes")
config = builder.build()

# Create database with custom config
from simple_embeddings_module import SEMDatabase
db = SEMDatabase(config=config)
```

### Provider Compatibility

SEM includes strict compatibility checking for provider switches:

```python
# Check if switching providers is safe
compatibility = builder.check_provider_compatibility(
    "sentence_transformers", model="all-MiniLM-L12-v2"
)

if compatibility.is_compatible:
    # Safe switch - no re-indexing needed
    db.switch_embedding_provider("sentence_transformers", model="all-MiniLM-L12-v2")
else:
    # Requires full re-indexing
    print(f"Migration required: {compatibility.reasons}")
```

## ğŸš¨ Compatibility Matrix

### âœ… Compatible Switches (No Re-indexing)
- Same model with different quantization (fp32 â†” fp16, q4, q8)
- Same model with different file formats (.bin â†” .safetensors)
- Sequence length within Â±20%

### âŒ Incompatible Switches (Full Re-indexing Required)
- Different embedding dimensions (384 vs 768)
- Different model families (MiniLM vs MPNet)
- Different model sizes (L6 vs L12, 400M vs 2B)
- Sequence length >20% difference

âš ï¸ **Best Effort Warning**: Even "compatible" switches may produce different results due to hardware differences (NVIDIA vs AMD vs Apple Silicon), RNG seeds, and numerical precision variations.

## ğŸ›ï¸ Performance

### Benchmarks (Apple Silicon M-series)

- **Model Loading**: ~2.5s (sentence-transformers/all-MiniLM-L6-v2)
- **Document Processing**: ~0.3s per document (chunking + embedding + storage)
- **Search Speed**: 0.4-2.1s depending on index size and query complexity
- **Memory Usage**: ~2GB for model + index size
- **Storage**: 44% smaller files than manual .tolist() with orjson compression

### Supported Hardware

- **Apple Silicon**: MPS (Metal Performance Shaders) acceleration
- **NVIDIA**: CUDA acceleration
- **AMD**: ROCm acceleration (Linux)
- **Intel/AMD**: CPU with optimized BLAS libraries
- **Cross-platform**: Same codebase works everywhere via PyTorch

## ğŸ”’ Security

- **No Pickle Files**: Uses secure orjson serialization exclusively
- **Atomic Writes**: Prevents data corruption during saves
- **Backup Rotation**: Configurable backup copies
- **Input Validation**: Comprehensive parameter validation
- **Safe Deserialization**: Only plain JSON data structures

## ğŸ“ Project Structure

```
simple-embeddings-module/
â”œâ”€â”€ src/simple_embeddings_module/
â”‚   â”œâ”€â”€ embeddings/           # Embedding providers
â”‚   â”‚   â”œâ”€â”€ mod_sentence_transformers.py
â”‚   â”‚   â””â”€â”€ mod_embeddings_base.py
â”‚   â”œâ”€â”€ chunking/            # Chunking strategies
â”‚   â”‚   â”œâ”€â”€ mod_text.py
â”‚   â”‚   â”œâ”€â”€ mod_code.py
â”‚   â”‚   â”œâ”€â”€ mod_chunking_ts.py
â”‚   â”‚   â”œâ”€â”€ mod_hierarchy_grouping.py    # NEW: Hierarchy-constrained grouping
â”‚   â”‚   â”œâ”€â”€ mod_hierarchy_integration.py # NEW: Integration utilities
â”‚   â”‚   â”œâ”€â”€ mod_chunk_mux.py
â”‚   â”‚   â””â”€â”€ mod_chunking_base.py
â”‚   â”œâ”€â”€ storage/             # Storage backends
â”‚   â”‚   â”œâ”€â”€ mod_local_disk.py
â”‚   â”‚   â””â”€â”€ mod_storage_base.py
â”‚   â”œâ”€â”€ serialization/       # Serialization providers
â”‚   â”‚   â”œâ”€â”€ mod_orjson.py
â”‚   â”‚   â””â”€â”€ mod_serialization_base.py
â”‚   â”œâ”€â”€ sem_core.py          # Core database engine
â”‚   â”œâ”€â”€ sem_cli.py           # Command-line interface
â”‚   â”œâ”€â”€ sem_config_builder.py # Configuration management
â”‚   â”œâ”€â”€ sem_module_reg.py    # Module registry system
â”‚   â””â”€â”€ sem_utils.py         # Utility functions
â”œâ”€â”€ demo/                    # Demonstration scripts
â”‚   â”œâ”€â”€ demo_semantic_chunking.py      # Basic semantic chunking demo
â”‚   â”œâ”€â”€ demo_ultimate_code_chunking.py # Multi-language chunking demo
â”‚   â”œâ”€â”€ demo_hierarchy_grouping.py     # NEW: Hierarchy grouping demo
â”‚   â”œâ”€â”€ example_code_search.py         # Semantic search example
â”‚   â”œâ”€â”€ demo_code_samples.py           # Sample code for demos
â”‚   â”œâ”€â”€ run_demos.sh                   # Demo runner script
â”‚   â””â”€â”€ README.md                      # Demo documentation
â”œâ”€â”€ test/                    # Test suite
â”‚   â”œâ”€â”€ smoke_test.py        # End-to-end functionality test
â”‚   â”œâ”€â”€ test_hierarchy_grouping.py     # NEW: Hierarchy grouping tests
â”‚   â”œâ”€â”€ test_semantic_chunking.py      # Semantic chunking tests
â”‚   â”œâ”€â”€ test_code_chunking_provider.py # Code chunking provider tests
â”‚   â”œâ”€â”€ test_lazy_loading.py           # Tree-sitter lazy loading tests
â”‚   â”œâ”€â”€ run_tests.sh         # Test runner
â”‚   â””â”€â”€ README.md            # Test documentation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ TODO.md
```

## ğŸ§ª Testing

SEM includes a comprehensive test suite to verify functionality:

```bash
# Run smoke test (recommended after installation)
python test/smoke_test.py

# Run specific tests
python test/test_hierarchy_grouping.py
python test/test_semantic_chunking.py
python test/test_code_chunking_provider.py
python test/test_lazy_loading.py

# Or run full test suite
./test/run_tests.sh
```

## ğŸ­ Demos

SEM includes comprehensive demos showcasing all capabilities:

```bash
# Run specific demos
python demo/demo_semantic_chunking.py
python demo/example_code_search.py
python demo/demo_ultimate_code_chunking.py
python demo/demo_hierarchy_grouping.py

# Or run all demos
./demo/run_demos.sh
```

The test suite verifies:
- Python imports and module loading
- CLI command availability
- GPU acceleration detection
- End-to-end semantic search functionality
- Hierarchy-constrained semantic grouping
- Search result quality
- Real embedding provider functionality (no mocks!)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Follow the existing code patterns and ABC base classes
4. Add tests for new functionality
5. Commit changes (`git commit -m 'Add amazing feature'`)
6. Push to branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Adding New Providers

To add a new embedding provider:

1. Create `mod_your_provider.py` in `src/simple_embeddings_module/embeddings/`
2. Inherit from `EmbeddingProviderBase`
3. Implement required abstract methods
4. Define `CONFIG_PARAMETERS` and `CAPABILITIES`
5. The module registry will auto-discover your provider

## ğŸ“„ License

Apache 2.0

## Main External Depends

- **HuggingFace sentence-transformers** for library and easily accessible embedding models
- **PyTorch** for universal GPU acceleration
- **orjson** for fast, secure JSON serialization

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/NeuralNotwerk/simple-embeddings-module/issues)
- **Discussions**: [GitHub Discussions](https://github.com/NeuralNotwerk/simple-embeddings-module/discussions)
- **Documentation**: See README.md and test/ directory